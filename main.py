import collections
import os
import string

import nltk

import settings
from learning.classifiers.sentiment_classifier import SNClassifier
from network.server import NetworkServer
from semantics.object_node import ObjectNode
from semantics.query_handler import QueryHandler
from semantics.semantic_network import SemanticNetwork


def clean_start():
    output_directory = settings.DATA_OUT
    for out_file in os.listdir(output_directory):
        out_file_path = os.path.join(output_directory, out_file)
        os.unlink(out_file_path)

def __in_master_list(master_list, value):
    for node in master_list:
        if node.get_value() == value:
            return True

    return False


if __name__ == "__main__":
    # # Erase all files in the out directory
    # GeneralUtils().clean_start()
    #
    # print("Begin")
    # print(nltk.word_tokenize("dog has a black coat"))
    # print(nltk.pos_tag(nltk.word_tokenize("dog has a black coat")))
    # print("-----------------------------------------------------\n\n")
    #
    # """
    #     The SNClassifier uses a Naive Bayes classifier to distinguish between a positive and
    #     a negative simple sentence. The SNClassifier uses a handcrafted corpus found in the
    #     data/sensets folder.
    # """
    sn_classifier = SNClassifier()
    sn_classifier.train()
    #
    # """
    #     Will load from a pickle if one exists.
    #     The NSBuilder builds XML network sets to train on. The output of this operation
    #     can be found in the out/network_set.xml file.
    # """
    # ns_builder = NSBuilder(sn_classifier).load()
    # ns_builder.create_standard_set()
    #
    # """
    #     The InputSequencer trains on the network set created by the previous operation.
    #     This operation uses a genetic algorithm in order to create function sequences that
    #     are used to parse new data.
    # """
    # input_sequencer = InputSequencer()
    # input_sequencer.train()
    #
    # """
    #     The QSBuilder builds question/statement lists to train on.
    #     This operation uses a presidential inaugural address corpus in conjunction with
    #     files pulled from NLP competition data sets.
    # """
    # # qs_builder = QSBuilder()
    # # qs_builder.create_standard_set()
    #
    # """
    #     The QSClassifier uses a Naive Bayes classifier to distinguish between a question
    #     and a statement. The QSClassifier trains on the data generated by the previous operation.
    # """
    # # qs_classifier = QSClassifier()
    # # qs_classifier.train()

    node_factory = ObjectNode.Factory()
    node_factory.add_value("alley")
    node_factory.add_value("cat")
    node_factory.add_relation("has")
    node_factory.add_relation("a")
    node_factory.add_relation_object("blue")
    node_factory.add_relation_object("coat")
    node = node_factory.build()

    node_factory2 = ObjectNode.Factory()
    node_factory2.add_value("blue")
    node_factory2.add_relation("is")
    node_factory2.add_relation("a")
    node_factory2.add_relation_object("color")
    node2 = node_factory2.build()

    node_factory3 = ObjectNode.Factory()
    node_factory3.add_value("blue")
    node_factory3.add_relation("is")
    node_factory3.add_relation("a")
    node_factory3.add_relation_object("color")
    node3 = node_factory3.build()

    test_network = SemanticNetwork(None)
    test_network.add_node(node)
    test_network.add_node(node2)
    # test_network.add_node(node3)
    test_network.print()

    query_handler = QueryHandler(test_network, sn_classifier, debug=True)

    network_server = NetworkServer()
    network_server.listen()
    while True:
        benson_query = network_server.get_data()
        network_server.send_data(query_handler.query(benson_query))

    for i in range(10):
        query = input("Enter a question: ")
        query_handler.query(query)

    exit()

    counter = collections.Counter(contender_values)
    counter.most_common()

    print(counter.most_common())
